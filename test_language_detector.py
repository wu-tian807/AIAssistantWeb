from utils.text_attachment.language_detector import LanguageDetector
import json

def print_result(title: str, result):
    """æ ¼å¼åŒ–æ‰“å°ç»“æœ"""
    print("\n" + "="*50)
    print(f"æµ‹è¯•åœºæ™¯ï¼š{title}")
    print("-"*50)
    if isinstance(result, dict):
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        print(result)

def test_language_detection():
    detector = LanguageDetector()
    
    # 1. å•å­—ç¬¦æµ‹è¯•
    single_char_texts = {
        "ä¸­æ–‡å­—": "æˆ‘",
        "è‹±æ–‡å­—": "A",
        "æ—¥æ–‡å‡å": "ã‚",
        "éŸ©æ–‡å­—": "í•œ",
        "ä¿„æ–‡å­—": "Ğ¯",
        "æ³°æ–‡å­—": "à¸",
        "ç‰¹æ®Šç¬¦å·": "â˜†",
        "æ•°å­—": "1",
        "ç©ºæ ¼": " ",
        "æ ‡ç‚¹": "ã€‚",
    }
    
    print("\n=== 1. å•å­—ç¬¦æµ‹è¯• ===")
    for desc, text in single_char_texts.items():
        result = detector.fallback_detect(text)
        print(f"\n{desc}:")
        print(f"æ–‡æœ¬: {text}")
        print(json.dumps(result, ensure_ascii=False, indent=2))

    # 2. æ··åˆè¯­è¨€è¾¹ç•Œæµ‹è¯•
    mixed_boundary_texts = {
        "ä¸­è‹±ç›¸è¿": [
            "Chineseä¸­æ–‡English",
            "Helloä¸–ç•ŒWorld",
            "ABCä¸­æ–‡DEF",
        ],
        "æ—¥è‹±ç›¸è¿": [
            "Japaneseã«ã»ã‚“ã”",
            "ã™ã—Sushi",
            "ABCã²ã‚‰ãŒãªDEF",
        ],
        "å¤šè¯­è¨€æ— ç©ºæ ¼": [
            "ä¸­æ–‡Englishí•œê¸€ã«ã»ã‚“ã”",
            "HelloĞœĞ¸Ñ€ä¸–ç•Œã“ã‚“ã«ã¡ã¯",
            "í…ŒìŠ¤íŠ¸Testæµ‹è¯•ãƒ†ã‚¹ãƒˆ",
        ],
        "ç‰¹æ®Šåˆ†éš”ç¬¦": [
            "ä¸­æ–‡|English|í•œê¸€|ã«ã»ã‚“ã”",
            "Hello-ä¸–ç•Œ-ĞœĞ¸Ñ€-ã“ã‚“ã«ã¡ã¯",
            "test_æµ‹è¯•_ãƒ†ã‚¹ãƒˆ_í…ŒìŠ¤íŠ¸",
        ],
    }
    
    print("\n=== 2. æ··åˆè¯­è¨€è¾¹ç•Œæµ‹è¯• ===")
    for category, texts in mixed_boundary_texts.items():
        print(f"\n{category}:")
        for text in texts:
            # å¯¹äºæ··åˆè¯­è¨€æ–‡æœ¬ï¼Œä½¿ç”¨è¾ƒå¤§çš„çª—å£
            result = detector.detect(text)
            print(f"\næ–‡æœ¬: {text}")
            print(json.dumps(result, ensure_ascii=False, indent=2))

    # 3. ç‰¹æ®Šæ ¼å¼æµ‹è¯•
    special_format_texts = {
        "é‡å¤æ¨¡å¼": {
            "ä¸­æ–‡é‡å¤": "å¥½å¥½å¥½å¥½å¥½å¥½å¥½å¥½å¥½å¥½",
            "è‹±æ–‡é‡å¤": "testtesttesttesttest",
            "æ··åˆé‡å¤": "ä¸­æ–‡Englishä¸­æ–‡English",
            "æ ‡ç‚¹é‡å¤": "ã€‚ã€‚ã€‚ã€‚ã€‚....",
        },
        "ç‰¹æ®Šåˆ†éš”": {
            "æ¢è¡Œåˆ†éš”": "Chinese\nä¸­æ–‡\nEnglish\nè‹±æ–‡",
            "åˆ¶è¡¨ç¬¦åˆ†éš”": "Chinese\tä¸­æ–‡\tEnglish\tè‹±æ–‡",
            "å¤šé‡ç©ºæ ¼": "Chinese    ä¸­æ–‡    English    è‹±æ–‡",
        },
        "ç‰¹æ®Šå­—ç¬¦": {
            "HTMLæ ‡ç­¾": "<div>è¿™æ˜¯ä¸­æ–‡</div><span>This is English</span>",
            "URL": "https://www.example.com/æµ‹è¯•/ãƒ†ã‚¹ãƒˆ",
            "é‚®ç®±": "test@example.comæµ‹è¯•@ç¤ºä¾‹.com",
            "è¡¨æƒ…ç¬¦å·": "HelloğŸ˜Šä½ å¥½ğŸ˜„ã“ã‚“ã«ã¡ã¯ğŸ˜",
        },
    }
    
    print("\n=== 3. ç‰¹æ®Šæ ¼å¼æµ‹è¯• ===")
    for category, texts in special_format_texts.items():
        print(f"\n{category}:")
        for desc, text in texts.items():
            result = detector.detect(text)
            print(f"\n{desc}:")
            print(f"æ–‡æœ¬: {text}")
            print(json.dumps(result, ensure_ascii=False, indent=2))

    # 4. è¯­è¨€ç‰¹å¾æµ‹è¯•
    language_feature_texts = {
        "ä¸­æ–‡ç‰¹å¾": {
            "ç¹ä½“": "æ¼¢å­—æª¢æ¸¬æ¸¬è©¦",
            "ç®€ä½“": "æ±‰å­—æ£€æµ‹æµ‹è¯•",
            "æ•°å­—ä¸­æ–‡": "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å",
            "ä¸­æ–‡æ ‡ç‚¹": "ã€è¿™æ˜¯ã€‘ã€Œæµ‹è¯•ã€ã€ä¾‹å­ã€‚",
        },
        "æ—¥æ–‡ç‰¹å¾": {
            "å¹³å‡å": "ã²ã‚‰ãŒãªã®ãƒ†ã‚¹ãƒˆ",
            "ç‰‡å‡å": "ã‚«ã‚¿ã‚«ãƒŠãƒãƒ†ã‚¹ãƒˆ",
            "æ—¥æ–‡æ±‰å­—": "æ¼¢å­—ãƒ†ã‚¹ãƒˆ",
            "æ—¥æ–‡æ ‡ç‚¹": "ã€Œã“ã‚Œã¯ã€ã€ãƒ†ã‚¹ãƒˆã€ã§ã™ã€‚",
        },
        "éŸ©æ–‡ç‰¹å¾": {
            "è°šæ–‡": "í•œê¸€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤",
            "éŸ©æ–‡æ±‰å­—": "éŸ“åœ‹èª í…ŒìŠ¤íŠ¸",
            "éŸ©æ–‡æ··åˆ": "í•œìì™€ í•œê¸€ì˜ í…ŒìŠ¤íŠ¸",
        },
        "æ¬§æ´²è¯­è¨€ç‰¹å¾": {
            "å¾·æ–‡å˜éŸ³": "Ãœber MÃ¼nchen lÃ¤uft Ã¤Ã¶Ã¼",
            "æ³•æ–‡é‡éŸ³": "VoilÃ  oÃ¹ est nÃ© cafÃ©",
            "è¥¿ç­ç‰™é‡éŸ³": "Â¿CÃ³mo estÃ¡ usted?",
            "ä¿„æ–‡å­—æ¯": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
        },
    }
    
    print("\n=== 4. è¯­è¨€ç‰¹å¾æµ‹è¯• ===")
    for category, texts in language_feature_texts.items():
        print(f"\n{category}:")
        for desc, text in texts.items():
            result = detector.detect(text)
            print(f"\n{desc}:")
            print(f"æ–‡æœ¬: {text}")
            print(json.dumps(result, ensure_ascii=False, indent=2))

    # 5. è¶…é•¿æ–‡æœ¬æµ‹è¯•
    long_texts = {
        "ä¸­æ–‡é•¿æ–‡æœ¬": "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ä¸­æ–‡æ–‡æœ¬ï¼Œ" * 50,
        "è‹±æ–‡é•¿æ–‡æœ¬": "This is a very long English text. " * 50,
        "æ··åˆé•¿æ–‡æœ¬": "è¿™æ˜¯ä¸€ä¸ªMixed Languageæ··åˆè¯­è¨€çš„Long Texté•¿æ–‡æœ¬ã€‚" * 30,
        "å¤šè¯­è¨€é•¿æ–‡æœ¬": "Englishä¸­æ–‡æ—¥æœ¬èªí•œê¸€" * 30,
    }
    
    print("\n=== 5. è¶…é•¿æ–‡æœ¬æµ‹è¯• ===")
    for desc, text in long_texts.items():
        result = detector.detect(text)
        print(f"\n{desc}:")
        print(f"æ–‡æœ¬é•¿åº¦: {len(text)}")
        print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    test_language_detection()